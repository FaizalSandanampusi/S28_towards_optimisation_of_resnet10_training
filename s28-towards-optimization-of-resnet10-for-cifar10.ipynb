{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training ResNet10 on CIFAR10: Unoptimized vs Optimized\n\n### In this notebook, we first train a ResNet10 model on the CIFAR10 dataset without any performance optimizations then we implement multiple strategies to speed up training by roughly 10x. ","metadata":{}},{"cell_type":"markdown","source":"## Section 1: Unoptimized Training\n\n#### In this section, we define the ResNet10 model along with a basic training loop using a small batch size and no additional optimizations.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.optim as optim\nimport time\nfrom torchvision import datasets, transforms\nfrom torchvision.datasets import CIFAR10\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Define a BasicBlock for ResNet-10\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out += self.shortcut(identity)\n        out = self.relu(out)\n        return out\n\n# Define the ResNet-10 model\nclass ResNet10(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(64, 64, stride=1)\n        self.layer2 = self._make_layer(64, 128, stride=2)\n        self.layer3 = self._make_layer(128, 256, stride=2)\n        self.layer4 = self._make_layer(256, 512, stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, num_classes)\n        self.init_weights()\n\n    def _make_layer(self, in_channels, out_channels, stride):\n        return nn.Sequential(BasicBlock(in_channels, out_channels, stride))\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                init.xavier_normal_(m.weight)\n                if m.bias is not None:\n                    init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n# Helper function for testing\ndef test_model(model, test_loader, device):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n    accuracy = 100 * correct / total\n    return accuracy\n\n# Data loading without optimizations\ntransform = transforms.ToTensor()\n\ntrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=False, num_workers=0)\n\ntest_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0)\n\n# Initialize the model, loss function, and optimizer (unoptimized)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_unoptim = ResNet10().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model_unoptim.parameters(), lr=0.001)  # Simple SGD\n\n# Training loop \nnum_epochs = 5\nprint(\"Starting training...\")\nstart_time = time.time()\n\nfor epoch in range(num_epochs):\n    model_unoptim.train()\n    epoch_start = time.time()\n    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=False)\n    for i, (inputs, labels) in enumerate(train_loop):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model_unoptim(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loop.set_postfix({\"Loss\": loss.item()})\n    \n    epoch_time = time.time() - epoch_start\n    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, Time: {epoch_time:.2f}s\")\n\ntotal_time_unoptimized = time.time() - start_time\nprint(f\"Total unoptimized training time: {total_time_unoptimized:.2f} seconds\")\n\ninitial_accuracy = test_model(model_unoptim, test_loader, device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:01:33.461449Z","iopub.execute_input":"2025-02-06T05:01:33.461788Z","iopub.status.idle":"2025-02-06T05:09:50.665038Z","shell.execute_reply.started":"2025-02-06T05:01:33.461759Z","shell.execute_reply":"2025-02-06T05:09:50.664053Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 1.5516, Time: 98.64s\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 1.3854, Time: 97.92s\n","output_type":"stream"},{"name":"stderr","text":"                                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 1.1749, Time: 97.66s\n","output_type":"stream"},{"name":"stderr","text":"                                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 0.9992, Time: 97.62s\n","output_type":"stream"},{"name":"stderr","text":"                                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 0.7867, Time: 97.67s\nTotal unoptimized training time: 489.52 seconds\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Section 2: Optimized Training  \n### Now we implement various optimization strategies:  \n### The strategies include:  \n\n### **DataLoader Optimizations**  \n- **Large Batch Size:** Set to 256 for efficient utilization of GPU resources.  \n- **Multiple Workers:** Utilizes 4 workers for parallel data loading.  \n- **Pinned Memory:** Enables faster data transfer from CPU to GPU by setting `pin_memory=True`.  \n\n### **Mixed Precision Training**  \n- **Automatic Mixed Precision:** Uses `torch.cuda.amp.autocast()` for faster computation on GPUs with Tensor Cores.  \n- **Gradient Scaling:** Applies `GradScaler()` for smoother accumulation and to prevent underflow issues during backpropagation.  \n\n### **Gradient Accumulation**  \n- **Simulated Larger Batch Size:** Divides the loss by `accumulation_steps=4` and accumulates gradients before updating weights.  \n- **Loss Normalization:** Divides the loss by accumulation steps for more stable training.  \n\n### **Modern Optimizer**  \n- **AdamW Optimizer:** Provides better generalization and faster convergence compared to traditional optimizers.  \n\n### **Learning Rate Scheduler**  \n- **OneCycleLR:** Dynamically adjusts the learning rate for stable and faster convergence during training.  \n\n### **Weight Sharing**  \n- **Shared Weights in Layer 1:** Reduces redundant computations and lowers parameter count by sharing weights across blocks in `layer1`.  ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import CIFAR10\nfrom tqdm import tqdm\nimport warnings\nimport torch.optim as optim\nfrom torch.amp import GradScaler, autocast\nimport time\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, share_weights=False):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.use_shortcut = stride == 1 and in_channels == out_channels\n        self.share_weights = share_weights\n\n    def forward(self, x):\n        identity = x if self.use_shortcut else None\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if identity is not None and not self.share_weights:\n            out += identity\n        out = self.relu(out)\n        return out\n\nclass ResNet10(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.in_channels = 64\n\n        # Initial layer\n        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n\n        # Layer 1 shares weights across blocks\n        self.layer1 = nn.Sequential(\n            BasicBlock(self.in_channels, 64, 1, share_weights=True)\n        )\n        self.in_channels = 64  # Ensure in_channels is updated\n\n        # Layers with unique parameters\n        self.layer2 = self._make_layer(128, 2)\n        self.layer3 = self._make_layer(256, 2)\n        self.layer4 = self._make_layer(512, 2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, out_channels, stride):\n        layer = BasicBlock(self.in_channels, out_channels, stride)\n        self.in_channels = out_channels  # Update in_channels after creating the layer\n        return nn.Sequential(layer)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\ndef initialize_weights(model):\n    \"\"\"\n    Apply Kaiming Initialization to Convolution and Linear layers\n    \"\"\"\n    for m in model.modules():\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n        elif isinstance(m, nn.BatchNorm2d):\n            nn.init.constant_(m.weight, 1)\n            nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n            nn.init.constant_(m.bias, 0)\n\n# Data loading\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4, pin_memory=True)\n\ntest_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch.cuda.amp\")\n\n# Initialize the model, loss function, and optimizer\noptim_model = ResNet10().cuda()\ninitialize_weights(optim_model)  # Apply Kaiming initialization\n\ncriterion = nn.CrossEntropyLoss().cuda()\noptimizer = optim.AdamW(optim_model.parameters(), lr=0.001, weight_decay=1e-4)\nscaler = GradScaler()\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=5)\n\n# Gradient Accumulation\naccumulation_steps = 4\n\nstart_time = time.time()\n# Training loop\nnum_epochs = 5\nstart_time = time.time()\nfor epoch in range(num_epochs):\n    optim_model.train()\n    epoch_start = time.time()\n    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n    \n    for i, (inputs, labels) in enumerate(train_loop):\n        inputs, labels = inputs.cuda(), labels.cuda()\n        with autocast(device_type='cuda'):\n            outputs = optim_model(inputs)\n            loss = criterion(outputs, labels) / accumulation_steps\n\n        scaler.scale(loss).backward()\n        \n        if (i + 1) % accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            optimizer.zero_grad()  # Move after accumulation\n\n    epoch_end = time.time()\n    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, Time: {epoch_end - epoch_start:.2f}s\")\n\ntotal_time_optimized = time.time() - start_time\nprint(f\"Total training time: {total_time_optimized:.2f}s\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:09:56.762213Z","iopub.execute_input":"2025-02-06T05:09:56.762677Z","iopub.status.idle":"2025-02-06T05:10:45.354303Z","shell.execute_reply.started":"2025-02-06T05:09:56.762638Z","shell.execute_reply":"2025-02-06T05:10:45.353259Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 0.4053, Time: 8.89s\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 0.2961, Time: 9.18s\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 0.2468, Time: 9.61s\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 0.1558, Time: 9.74s\n","output_type":"stream"},{"name":"stderr","text":"                                                            ","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 0.2252, Time: 9.54s\nTotal training time: 46.98s\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"optimized_accuracy = test_model(optim_model, test_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:12:54.496781Z","iopub.execute_input":"2025-02-06T05:12:54.497119Z","iopub.status.idle":"2025-02-06T05:12:56.189540Z","shell.execute_reply.started":"2025-02-06T05:12:54.497087Z","shell.execute_reply":"2025-02-06T05:12:56.188556Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"optimized_accuracy = test_model(optim_model, test_loader, device)\n\n# ----------------------\n# Results Comparison\n# ----------------------\n\nprint(\"\\nPerformance Comparison:\")\nprint(f\"{'Implementation':<20} | {'Accuracy (%)':<15} | {'Training Time (s)':<20}\")\nprint(f\"{'Initial':<20} | {initial_accuracy:.2f} | {total_time_unoptimized:.2f}\")\nprint(f\"{'Optimized':<20} | {optimized_accuracy:.2f} | {total_time_optimized:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:12:58.522449Z","iopub.execute_input":"2025-02-06T05:12:58.522761Z","iopub.status.idle":"2025-02-06T05:13:00.132543Z","shell.execute_reply.started":"2025-02-06T05:12:58.522735Z","shell.execute_reply":"2025-02-06T05:13:00.131667Z"}},"outputs":[{"name":"stdout","text":"\nPerformance Comparison:\nImplementation       | Accuracy (%)    | Training Time (s)   \nInitial              | 65.65 | 489.52\nOptimized            | 66.67 | 46.98\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}